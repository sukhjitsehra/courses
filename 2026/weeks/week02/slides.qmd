---
title: "What is Statistical Learning?"
# subtitle: "<font style='font-size:0.6em;'>Programs that improve at a task through experience, measured by performance </font>"
author: </br></br> Dr. Sukhjit Singh Sehra </br> <a href="https://www.linkedin.com/in/sukhjitsehra" target="_blank">
    <img src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons/icons/linkedin.svg" 
         alt="LinkedIn" width="18" style="vertical-align:middle;">
  </a>
  &nbsp;
  <a href="https://github.com/sukhjitsehra" target="_blank">
    <img src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons/icons/github.svg" 
         alt="GitHub" width="18" style="vertical-align:middle;">
  </a>

institute:  <font style='font-size:2em;'>Department of Computer Science and Physics </br> Wilfrid Laurier University, Waterloo </font>
# date: 14 January 2026
# date-meta: 14 March 2023
date-format: "DD MMM YYYY"
toc: true
toc-depth: 1
toc-title: "What we will cover today:"
center-title-slide: false
from: markdown+emoji
format:
  revealjs: 
    slide-number: c/t
    # height: 1080
    # width: 1920
    css: [../../../css/syntax-highlight.css, ../../../css/custom.css, /css/pacman.css, ../../../css/styles_slides.css]
    chalkboard: 
      buttons: TRUE
    fig-responsive: true
    smaller: true
    theme: [../../../css/monash.scss]
    mouse-wheel: false
    preview-links: auto
    progress: true
    code-link: true
    code-overflow: wrap
    preload-iframes: true
    fig-align: "center"
    page-layout: full
    logo: /figures/icons/laurier-short.png
    footer: 'CP322: Machine Learning'
execute: 
  eval: true
  echo: true
---


## Abstract

We give a number of examples illustrating what statistical learning is about.

The emphasis is not on particular methods but on the foundational concepts.

**Bottom line:** This sets up the core ideas that everything else will build on.


## Overview

- Models
- Prediction & inference
- Accuracy & interpretability
- Bias & Variance
- Regression & classification

**Bottom line:** This sets the stage for the remainder of the course.


## Example: Advertising (1)

- We are hired to increase the sales of a particular product.
- We are given the `Advertising` data set:
  - `sales` for 200 different markets.
  - advertising budgets for `TV`, `radio` and `newspaper`.
- Our clients cannot directly influence the sales.
- They can control the advertising budgets.
- We need to determine whether there is an association  
  between advertising and sales.

**Bottom line:** This is a typical scenario.


## Example: Advertising (2)

![Advertising scatterplots](2_1){width=60%}

- The plots display `sales` as a function of the advertising budgets.
- Each plot is a _scatter plot_ of `sales` versus a budget.
- We have overlaid a _least squares line_ fit on each plot.

**Bottom line:** We can already spot some promising relationships.


## Example: Advertising (3)

- The advertising budgets are _input variables_.
- We denote the input variables with $X$.
- We use subscripts to distinguish them.
- For example, $X_1 = \texttt{TV}$, $X_2 = \texttt{radio}$, $X_3 = \texttt{newspaper}$.
- The input variables are also called _predictors_, _independent variables_,  
  _features_ or simply _variables_.
- `sales` is the _output variable_.
- Output variables are also called _dependent variables_ or _responses_.
- We denote the output variables with $Y$.

**Bottom line:** Do not get confused by different names.


## Models

- Suppose we observe a _quantitative_ response $Y$  
  and $p$ different predictors, $X_1, X_2, \dots, X_p$.
- We assume that there is some relationship between $Y$ and $X$.
- We can write this in the very general form
  $$
  Y = f(X) + \epsilon
  $$
- Here $f$ is a fixed but unknown function of $X = (X_1, X_2, \dots, X_p)$.
- $\epsilon$ is a random error term, independent of $X$ with mean zero.
- $f$ represents the _systematic_ information that $X$ provides about $Y$.

**Bottom line:** This is the most general form of a _model_.


## Example: Income (1)

![Income vs education](2_2){width=50%}

- The figure shows a scatter plot of `income` versus `years of education`.
- The blue curve is the true functional relationship $f(X) = f(X_1)$.
- The vertical lines represent the error terms $\epsilon$.

**Bottom line:** This is a simulated data set.


## Example: Income (2)

![Income vs education and seniority](2_3){width=40%}

- The figure shows a 3D scatter plot of `income` versus `years of education`  
  and `seniority`.
- The blue surface is the true functional relationship $f(X) = f(X_1, X_2)$.
- The vertical lines represent the error terms $\epsilon$.

**Bottom line:** Visualising higher dimensions is hard.


## What is Statistical Learning?

In essence, statistical learning refers to a set of approaches for estimating the function $f$.

In this lecture we cover the key theoretical concepts that arise in estimating $f$, as well as techniques for evaluating the estimates obtained.

**Bottom line:** All of this applies equally to what people call “machine learning”.


## Why estimate $\mathbf{f}$?

There are two main reasons why we wish to estimate $f$.

1. _Prediction_: assuming the inputs $X$ are readily available, we simply want to
   predict
   $$
   \hat{Y} = \hat{f}(X)
   $$
   without being interested in the exact form of $\hat{f}$.
2. _Inference_: we want to understand the functional relationship
   $$
   Y = Y(X_1, X_2, \dots, X_p)
   $$
   between the predictors and the response.

**Bottom line:** Prediction and inference are related but not the same goal.


## Prediction

- Often the inputs $X$ are readily available but the response $Y$ is not.
- Since the error term $\epsilon$ averages to zero, we can predict
  $$
  \hat{Y} = \hat{f}(X)
  $$
  where $\hat{f}$ is the estimate of $f$ and $\hat{Y}$ is the resulting prediction of $Y$.
- In this scenario we are not interested in the exact form of $\hat{f}$.
- We are mostly concerned with the accuracy of $\hat{Y}$.

**Bottom line:** Here $\hat{f}$ is often treated as a _black box_. It does not have to be.


## Prediction Accuracy (1)

- The accuracy of $\hat{Y}$ depends on two quantities:
  1. the _reducible error_
  2. the _irreducible error_
- The reducible error is due to $\hat{f}$ not being a perfect estimate of $f`.  
  (We can potentially come up with better statistical learning techniques to improve $\hat{f}$.)
- Even if our estimate of the relationship was perfect ($\hat{f} = f$),  
  our estimate $\hat{Y}$ would still have an error.
- This irreducible error is due to our _observations_ not being perfect  
  (the error term $\epsilon$).

**Bottom line:** You have to understand what you can improve and what you cannot.


## Prediction Accuracy (2)

- Assuming that $\hat{f}$ and $X$ are fixed, we can show that

$$
\begin{aligned}
E[(Y - \hat{Y})^2]
&= E[(f(X) + \epsilon - \hat{f}(X))^2] \\
&= \underbrace{E[(f(X) - \hat{f}(X))^2]}_{\text{Reducible}}
+ \underbrace{\operatorname{Var}(\epsilon)}_{\text{Irreducible)}
\end{aligned}
$$

where $E[(Y - \hat{Y})^2]$ is the average, or _expected value_, of the squared  
difference between the predicted and the true value of $Y$, and $\operatorname{Var}(\epsilon)$  
is the _variance_ of the error term $\epsilon$.

- The average, or expected value, $E[z]$ is often written as $\overline{z}$ or $\langle z \rangle$.
- The variance is then
  $$
  \operatorname{Var}(z)
  = \langle (z - \overline{z})^2 \rangle
  = \langle z^2 \rangle - {\langle z \rangle}^2
  = \overline{z^2} - \overline{z}^2.
  $$

**Bottom line:** All techniques we learn aim to estimate $f$ while minimising the reducible error.


## Inference

- We are often interested in _how_ $X_1, X_2, \dots, X_p$ affect $Y$.
- We want to estimate $f$, but not necessarily predict $Y$.
- The aim is to understand the _functional relationship_ between $Y$ and $X$.

**Bottom line:** In this setting we cannot treat $\hat{f}$ as a black box.


## Inference: Questions

- **Which predictors are associated with the response?**  
  We want to identify the _important_ predictors.
- **What is the relationship between the response and each predictor?**  
  For example, predictors can have positive or negative influence on the response.  
  Any functional relationship can occur, possibly depending on the values of the other predictors.
- **Can the relationship between $Y$ and each predictor be summarised as a linear equation?**  
  It greatly simplifies things when the relationship can be captured by a linear model.  
  Historically, models have mostly been chosen to be linear. The true relationships are very rarely linear.

**Bottom line:** Linear is convenient, rarely exact.


## How do we estimate $\mathbf{f}$?

::: columns
::: column
- We have $n$ _observations_ of $X$ and $Y$.
- In the plots on the right $n = 30$.
- This is the _training data_.
- We use these observations to _train_, or _teach_, our model how to estimate $f$.
- The goal is to find a function $\hat{f}$ such that $Y \approx \hat{f}(X)$  
  for each observation $(X, Y)$.
:::

::: column
![Income vs education](2_2)

![Income vs education and seniority](2_3){width=90%}
:::
:::

**Bottom line:** Statistical learning methods are either _parametric_ or _non-parametric_.


## Parametric Methods

1. **Make an assumption about the functional form of $f$.**  
   A very simple assumption is a linear model:
   $$
   f(X) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p.
   $$
   With a linear model we only need to estimate $p + 1$ parameters.  
   We will cover linear models in great detail in the next lecture.
2. **Use the training data to fit the model.**  
   For a linear model we need to estimate the parameters  
   $\beta_0, \beta_1, \beta_2, \dots, \beta_p$ such that
   $$
   Y \approx \hat{\beta}_0 + \hat{\beta}_1 X_1 + \hat{\beta}_2 X_2
      + \dots + \hat{\beta}_p X_p.
   $$

**Bottom line:** Non-linear models are possible, but harder to fit.


## Linear Model Example

![Linear model](2_4){width=50%}

\[
\texttt{income} \approx \hat{\beta}_0
+ \hat{\beta}_1 \times \texttt{years of education}
+ \hat{\beta}_2 \times \texttt{seniority}
\]


## Non-parametric Methods

1. **No assumption** about the functional form of $f$ is made.  
2. **Seek an estimate of $f$ that smoothly approaches the data points**  
   in the training data.

Pros:

- avoids choosing potentially very wrong models
- can approximate complex functional relationships

Cons:

- often needs many more observations for the fit
- prone to over-fitting

**Bottom line:** Non-parametric methods can still have many parameters in practice.


## Non-parametric Example (1)

![Spline fit](2_5){width=50%}

Approximation to $f$ to predict `income` using a _thin-plate spline_.

**Bottom line:** Better fit to the training data than the linear model.


## Non-parametric Example (2)

![Overfitting spline](2_6){width=50%}

A less smooth spline fit that fits the training data perfectly.

**Bottom line:** This is an example of _over-fitting_.


## Accuracy versus Interpretability Trade-off (1)

- **Why would we choose a less flexible approach?**
  - Interpretability is often desirable (inference).
  - In a linear model we can directly infer whether a relationship is positive or negative from the sign of the $\beta$ coefficient.
- **Why would we choose a more flexible approach?**
  - We might be mostly interested in the accuracy of our estimate of $f$ (prediction).
  - In the extreme this is a _black box_ approach where we do not care about interpretation at all.

**Bottom line:** This is a _trade-off_; we have to decide what matters for the task.


## Accuracy versus Interpretability Trade-off (2)

![Accuracy vs interpretability](2_7){width=60%}

In general, more flexible statistical learning methods have lower interpretability.

**Bottom line:** We will learn more about the methods mentioned in this plot later.


## Supervised versus Unsupervised Learning

- **Supervised Learning**
  - We have $n$ observations of the $p$ predictors $X_j$ _and_ the corresponding response $Y$.
  - We can fit a model to the training data.
  - The _supervision_ comes from the presence of the known response $Y$ in the training data.
- **Unsupervised Learning**
  - We have $n$ observations of the $p$ predictors $X_j$ but no access to the corresponding response $Y$.
  - All we can do is look for _patterns_ in the training data.

**Bottom line:** This distinction is not always clear-cut in practice.


## Unsupervised Example

![Clustering example](2_8){width=60%}

Clustering data involving three groups. Left: little overlap. Right: larger overlap.

**Bottom line:** For large data sets we need automated clustering.


## Regression versus Classification

- Variables, in particular responses, can be _quantitative_ or _qualitative_.

**Quantitative responses – regression problems**

- gas mileage, crime rate, sales

**Qualitative responses – classification problems**

- sex (male, female), default on debt (yes, no), brand (Apple, Samsung, Blackberry)

**Bottom line:** In terms of statistical learning methods the distinction is not perfectly sharp.


## Assessing Model Accuracy

- In this course we introduce a wide range of statistical learning methods.
- Why not just teach the “best” method?
- No single method dominates all others over all possible data sets.
- It is our task to decide which method produces the best results on a given data set.
- We therefore need a way to _measure_ how well we are doing.

**Bottom line:** There is no free lunch in statistical learning.


## Measuring Fit Quality (1)

- How can we measure the quality of a fit?
- We need to quantify how close our estimate $\hat{Y} = \hat{f}(X)$ is to the true values $Y$.
- The difference between the two is a measure of this, but the sign does not matter.
- We therefore use the _mean squared error_ (MSE):

$$
\text{MSE}
= \frac{1}{n}\sum_{i=1}^n (y_i - \hat{f}(x_i))^2
= E[(Y - \hat{f}(X))^2]
= \langle (Y - \hat{f}(X))^2 \rangle.
$$

**Bottom line:** Other choices for the _loss function_ are possible, this is the most common one.


## Measuring Fit Quality (2)

- We could simply compute the MSE on the training data.
- Most fit, or learning, methods work by minimising the MSE  
  (or another suitable _loss function_) on the training data.
- However, we want to make predictions on _future_ observations not available at training time.
- A better measure is the MSE on a _test data set_ that is _independent_ of the training set.
- We can obtain an independent test data set by using only a subset of the data for training and the remainder for testing (_cross validation_).

**Bottom line:** The training data MSE overestimates the real-world fit quality.


## Measuring Fit Quality: Example 1

![Fit quality example 1](2_9){width=60%}

Left: true $f$ (black), linear regression (orange), and spline fits (blue, green).  
Right: training (grey) and test (red) MSE.

**Bottom line:** The most flexible model over-fits the training data.


## Measuring Fit Quality: Example 2

![Fit quality example 2](2_10){width=60%}

Left: true $f$ (black), linear regression (orange), and spline fits (blue, green).  
Right: training (grey) and test (red) MSE.

**Bottom line:** When the truth is close to linear, linear regression does well.


## Measuring Fit Quality: Example 3

![Fit quality example 3](2_11){width=60%}

Left: true $f$ (black), linear regression (orange), and spline fits (blue, green).  
Right: training (grey) and test (red) MSE.

**Bottom line:** When the truth is highly non-linear, linear regression does badly.


## The Bias–Variance Trade-off (1)

- Denote the observations from a test data set as $(x_0, y_0)$.
- The test data MSE is
  $$
  \text{MSE}_\text{test} = \langle (y_0 - \hat{f}(x_0))^2 \rangle.
  $$
- We can show that this decomposes as

$$
\begin{aligned}
\langle (y_0 - \hat{f}(x_0))^2 \rangle
&=
\underbrace{\operatorname{Var}(\hat{f}(x_0))
+ \big(\operatorname{Bias}(\hat{f}(x_0))\big)^2}_{\text{Reducible}}
+ \underbrace{\operatorname{Var}(\epsilon)}_{\text{Irreducible}}
\end{aligned}
$$

where the _bias_ is

$$
\operatorname{Bias}(\hat{f}(x_0))
= \langle \hat{f}(x_0) \rangle - \langle f(x_0) \rangle.
$$

**Bottom line:** In practice we do not know the true $f$ and cannot compute the bias exactly.


## The Bias–Variance Trade-off (2)

Given

$$
\langle (y_0 - \hat{f}(x_0))^2 \rangle
=
\underbrace{\operatorname{Var}(\hat{f}(x_0))
+ \big(\operatorname{Bias}(\hat{f}(x_0))\big)^2}_{\text{Reducible}}
+ \underbrace{\operatorname{Var}(\epsilon)}_{\text{Irreducible}},
$$

- we want to minimise both the variance of $\hat{f}(x_0)$ _and_ the bias.
- This is often not possible at the same time.
- Highly flexible methods can eliminate the bias.
- That does not mean they perform better at prediction.

**Bottom line:** The bias–variance trade-off translates into a flexibility trade-off.


## The Bias–Variance Trade-off (3)

![Bias-variance curves](2_12){width=80%}

The plots correspond to the three example data sets we saw before  
(non-linear, almost linear, highly non-linear).


## The Classification Setting

- As before, we want to estimate $f$ from the training observations
  $$
  \{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\}
  $$
  and train a _classifier_.
- Now the responses $y_i$ are _qualitative_.
- The $y_i$ are _labels_ denoting the different _classes_ the output variable can belong to.
- For example
  $$
  y = \texttt{origin},\quad
  \texttt{origin} \in \{\texttt{US}, \texttt{EU}, \texttt{JP}\}.
  $$

**Bottom line:** Classifiers usually predict class _probabilities_ from observations.


## Probabilities (1)

- We need to formalise our notion of _probability_.

> We use probabilities to quantitatively express  
> our relative beliefs in various propositions.

- Mathematicians rightly ask: “How can you formalise a _belief_?”
- Some argue we can only speak of the _frequency_ at which events occur in an _ensemble_.
- Often we use observed frequencies to determine probabilities.
- Sometimes the concept of an ensemble does not make much sense.
- We take a pragmatic approach without getting too philosophical.

**Bottom line:** Do not get dragged into endless debates with hard-core frequentists.


## Probabilities: Cook’s Rules

- We clearly need a _transitive_ property: if we believe (a) more than (b)  
  and (b) more than (c) then we must believe (a) more than (c):
  $$
  P(a) > P(b) \land P(b) > P(c) \implies P(a) > P(c),\quad P \in \mathbb{R}.
  $$
- If we specify how much we believe $X$ to be true we have implicitly specified how much we believe $X$ to be false. With the conventional norm of one:
  $$
  P(X \mid I) + P(\bar{X} \mid I) = 1.
  $$
- If we specify how much we believe $X$ is true and then how much we believe $Y$ is true _given_ $X$, we have implicitly specified how much we believe $Y$ _and_ $X$ are true:
  $$
  P(Y, X \mid I) = P(Y \mid X, I)\, P(X \mid I).
  $$

**Bottom line:** These give the quantitative axioms for logical, consistent reasoning under uncertainty.


## Corollaries

**Bayes’ theorem**

$$
P(Y \mid X, I) = \frac{P(X \mid Y, I)\, P(Y \mid I)}{P(X \mid I)}.
$$

**Marginalisation**

$$
P(Y \mid I) = \int_{-\infty}^{+\infty} P(Y, X \mid I)\, dX.
$$

Here $I$ denotes our background knowledge and assumptions.

**Bottom line:** This is the backbone of Bayesian reasoning.


## Bayes’ Theorem (1)

- The power of Bayes’ theorem is that it turns one _conditional probability_ into another.
- It relates $P(X \mid Y, I)$ to $P(Y \mid X, I)$.
- The importance is clearer when we replace $Y$ with _hypothesis_ and $X$ with _data_:

$$
P(\text{hypothesis} \mid \text{data}, I)
\propto
P(\text{data} \mid \text{hypothesis}, I)\,
P(\text{hypothesis} \mid I).
$$

- Here we omit the normalisation factor.

**Bottom line:** This relates the probability we care about to one we can hope to measure.


## Bayes’ Theorem (2)

$$
P(Y \mid X, I) = \frac{P(X \mid Y, I)\, P(Y \mid I)}{P(X \mid I)}.
$$

::: {.callout-note title=""}
| **Term**         | **Name**               |
|------------------|------------------------|
| $P(Y \mid X, I)$ | posterior probability  |
| $P(X \mid Y, I)$ | likelihood             |
| $P(Y \mid I)$    | prior probability      |
| $P(X \mid I)$    | evidence               |
:::

**Bottom line:** Statistical learning methods approximate (or indirectly target) the posterior probability.


## Classifier Accuracy

- The concepts we developed for regression also apply to classification.
- We need a way to quantify classifier accuracy.
- Define the _training error rate_:
  $$
  \frac{1}{n}\sum_{i=1}^n I(y_i \ne \hat{y}_i)
  = \langle I(y_i \ne \hat{y}_i) \rangle,
  $$
  where
  $$
  I(y_i \ne \hat{y}_i) =
  \begin{cases}
  1 & y_i \ne \hat{y}_i \\
  0 & y_i = \hat{y}_i
  \end{cases}
  $$
- With $(x_0, y_0)$ denoting the test observations, the _test error rate_ is
  $$
  \langle I(y_0 \ne \hat{y}_0) \rangle.
  $$

**Bottom line:** A good classifier minimises the test error rate.


## The Bayes Classifier

- The best possible classifier minimises the test error rate on average.
- It assigns each observation to the most likely class given the predictors.
- The probability that $Y$ belongs to class $j$, given $x_0$, is
  $$
  P(Y = j \mid X = x_0).
  $$
- In a scenario with two classes (say `Up` and `Down`) this classifier predicts  
  $Y = \texttt{Up}$ if
  $$
  P(Y = \texttt{Up} \mid X = x_0) > 0.5
  $$
  and $Y = \texttt{Down}$ otherwise.
- This is the _Bayes classifier_ and it is ideal.
- On simulated data we can compute this probability exactly.

**Bottom line:** No method can beat the proper Bayesian posterior on average.


## The Bayes Decision Boundary

![Bayes decision boundary](2_13){width=45%}

A simulated data set with 100 observations. The dashed line is the _Bayes decision boundary_.


## K-Nearest Neighbours (KNN) (1)

1. Given an integer $K$ and a test observation $x_0$, find the $K$ observations $x_i$  
   in the training data set that are closest to $x_0$, denoted by $\mathcal{N}_0$.
2. Estimate the conditional probabilities from the relative frequencies of  
   the classes in $\mathcal{N}_0$ (likelihoods):
   $$
   P(X = x_0 \mid Y = j)
   = \frac{1}{K}\sum_{i \in \mathcal{N}_0} I(y_i = j).
   $$
3. Use Bayes’ theorem to compute the conditional _posterior probability_:
   $$
   P(Y = j \mid X = x_0)
   = \frac{P(X = x_0 \mid Y = j)\, P(Y = j)}{P(X = x_0)}.
   $$

**Bottom line:** KNN _estimates_ $P(Y \mid X)$. Its _flexibility_ scales roughly like $1/K$.


## K-Nearest Neighbours (2)

![KNN example 1](2_14){width=70%}

The KNN approach with $K = 3$. The black “x” in the left panel is a test observation.


## K-Nearest Neighbours (3)

![KNN decision boundaries](2_16){width=70%}

A comparison of decision boundaries with $K=1$ and $K=100$.


## K-Nearest Neighbours (4)

![KNN vs Bayes boundary](2_15){width=40%}

Comparing to the Bayes decision boundary, $K = 10$ seems a good choice.


## K-Nearest Neighbours (5)

![KNN flexibility vs performance](2_17){width=50%}

Flexibility versus model performance on training and test data sets.

**Bottom line:** The same bias–variance trade-off appears again through the choice of $K$.
```

## What is Statistical Learning?

- **Input variables**: $X_1, \cdots, X_p$  
Also known as *predictors, features, independent variables*.
- **Output variable**: $Y$  
Also known as *response or dependent variable*.

We assume there is some relationship between
$Y$ and $X = \left( X_1, \cdots, X_p \right)$, 
which we write as: $Y = f(X) + \epsilon$, where $\epsilon$ is a random **error term** which is **independent** from $X$ and has mean zero; and, $f$ represents the **systematic information**  that $X$ provides about $Y$.

![](img/02-prediction.jpg)

In essence, statistical learning deals with **different approaches to estimate $f$**.

## Why estimate $f$?

Two main reasons to estimate $f$:

### Prediction

- Predict $Y$ using a set of inputs $X$ .
- Representation: $\hat{Y}= \hat{f}(X)$,
where $\hat{f}$ represents our *estimate* for $f$,
and $\hat{Y}$ our *prediction* for $Y$ .*

- In this setting, $\hat{f}$ is often treated as a
**black-box**, meaning we don't mind not knowing 
the exact form of $\hat{f}$, if it generates
accurate predictions for $Y$ .

- $\hat{Y}$'s accuracy depends on:
    - **Reducible error** 
        - Due to $\hat{f}$ not being a perfect estimate for $f$.
        - **Can be reduced** by using a proper statistical learning technique.
    - **Irreducible error** 
        - Due to $\epsilon$ and its variability.
        - $\epsilon$ is independent from $X$, so  no matter
        how well we estimate $f$, we can't reduce this error.

- The quantity $\epsilon$ may contain **unmeasured variables** 
useful for predicting $Y$; or, may contain **unmeasure variation**,
so no prediction model will be perfect.

- Mathematical form, after choosing predictors $X$ and an estimate
$\hat{f}$:

$$
E( Y - \hat{Y} )^2 =
E(f(X) + \epsilon - \hat{f}(X))^2 =
\underbrace{[f(X) - \hat{f}(X)]^2}_{reducible} +
\underbrace{\text{ Var}(\epsilon)}_{irreducible}\; .
$$ 

In practice, we almost always don't know how 
$\epsilon$'s variability affects our model,
so, in this book, we will focus on techniques for estimating $f$ .

### Inference

In this case, we are interested in **understanding the association** 
between $Y$ and $X_1, \cdots, X_p$.

- For example:
    - *Which predictors are most associated with response?*
    - *What is the relationship between the response and each predictor?*
    - *Can such relationship be summarized via a linear equation, or is it more complex?*

The exact form of $\hat{f}$ is required.

Linear models allow for easier interpretability, but 
can lack in prediction accuracy; while,
non-linear models can be more accurate, but less interpretable.

## How do we estimate $f$ ?

- First, let's agree on some conventions:
    - $n$ : Number of observations.
    - $x_{ij}$: Value of the $j\text{th}$ predictor, for $i\text{th}$ observation.
    - $y_i$ : Response variable for $i\text{th}$ observation.
    - **Training data**: 
        - Set of observations.
        - Used to esmitate $f$.
        - $\left\{ (x_1, y_1), \cdots, (x_n, y_n) \right\}$,
        where $x_i = (x_{i1}, \cdots, x_{ip})^T$ .
  
- Goal: Find a function $\hat{f}$ such that $Y\approx\hat{f}(X)$ 
for any observation $(X,Y)$ .

- Most statistical methods for achieving this goal can be
characterized as either **parametric** or **non-parametric**.

### Parametric methods

- Steps:
    1. Make an assumption about the **form** of $f$. \
    It could be linear 
    ($f(X) = \beta_0 + \beta_1 X_1 + \cdot + \beta_p X_p,$ 
    parameters $\beta_0, \cdots, \beta_p$ to be estimated) or not.
    1. The model has been selected. \
    Now, we need a procedure to **fit** the model using the training data. \
    The most common of such fitting procedures is called 
    **(ordinary) least squares**.

- Via these steps, the problem of estimating $f$ has been reduced
to a problem of estimating a **set of  parameters**.

- We can make the models more **flexible** via considering a 
greater number of parameters, but, this can lead to **overfitting the data**, that is, following the errors/noise too closely, 
which will not yield accurate estimates of the response for 
observations outside of the original training data. \

### Non-parametric methods

- No assumptions about the form of $f$ are made.
- Instead, we seek an estimate of $f$ which 
that gets as close to the data point as possible.
- Has the potential to fit a wider range of possible forms for $f$.
- Tipically requires a very large number of observations
(compared to paramatric approach) in order to accurately estimate $f$.


## The trade-off between prediction accuracy and model interpretability

We've seen that parametric models are usually restrictive; and,
non-parametric models, flexible. However:

- **Restrictive** models are usually more **interpretable**,
so they are useful for inference.
- **Flexible** models can be difficult to interpret, due to
the complexity of $\hat{f}$.

Despite this, we will often obtain **more accurate predictions** 
usinf a **less flexible method**, due to the potential for
*overfitting the data* in highly flexible models.

## Supervised vs Unsupervised Learning

In **supervised learning**, we wish to fit a model
that relates inputs/predictors to some output.

In **unsupervised learning**, we lack a reponse/variable
to predict. Instead, we seek to understand the relationships
between the variables or between the observations.

There are instances where a mix of such methods is required
(**semi-supervised learning problems**), but such topic
will not be covered in this book.

## Regression vs Classification problems

- If the **response is** ...
    - **Quantitative**, then, it's a regression problem.
    - **Categorical**, then, it's a classification problem.

- Most of the methods covered in this book can be applied
regardless of the predictor variable type, but the categorical
variables will require some pre-processing.

## Assessing model accuracy

- There is no **best method** for Statistical Learning, 
the method's efficacy can depend on the data set.

- For a specific data set, **how do we select the best Statistics approach**?

## Measuring the quality of fit

- The performance of a statistical learning method can be evaluated
comparing the predictions of the model, with their true/real response.

- Most commonly used measure for this:
    - **Mean squared error** 
    - $\text{ MSE } = \dfrac{1}{n}\displaystyle{ \sum_{i=1}^{n}(y_i - \hat{f}(x_i))^2 }$ 
    - Small MSE means that the predicted and the true responses are very close.

- We want the model to accurately predict **unseen data** (testing data),
not so much the training data, where the response is already known.

- The *best* model will be the one which produces the **lowest test MSE**,
not the lowest training MSE.

- It's **not true** that the model with lowest training MSE will also
have the lowest test MSE.

![](img/02-train-test-MSE.jpg)

- **Fundamental property**: For any data set and any statistical learning method used, as the flexibility of the statistical learning method increases:
    - The training MSE decreases monotonically.
    - The test MSE graph has a *U*-shape.

> As model flexibility increases, training MSE will decrease,
> but the test MSE **may not**.

- Small training MSE but big test MSE implies having overfitted the data.

- Regardless of overfitting or not, we almost always expect 
$\text{training MSE } < \text{ testing MSE }$, beacuse most statistical learning methods seek to minimize the training MSE.

- Estimating test MSE is very difficult, usually because lack of data.
Later in this book, we'll discuss approaches to estimate the 
**mininum point** for the test MSE curve.

## The Bias-Variance Trade-off

- **Definition**: The **expected test MSE** at $x_0$ 
($E(y_0 - \hat{f}(x_0))^2$) refers to 
the averga test MSE that we would obtain after repeatedly estimating
$f$ using a large number of training sets, and tested each esimate
at $x_0$.

- **Definition**: The variance of a statistical learning method which
produces an estimate $\hat{f}$ refers to how the estimate function
changes, for different training sets.

- **Definition**: **Bias** refers to the error generated by approximating
a possibly complicated model (like in real-life usually), by a much simpler one ... (how $f$ and the possibles $\hat{f}$ *differ*).

- As a general rule, the **more flexible** a statistical method, 
the **higher its variance** and **lower its bias**.

- For any given value $x_0$, the following can be proved:

$$
E(y_0 - \hat{f}(x_0))^2 = \text{Var}(\hat{f}(x_0)) + \text{Bias}(\hat{f}(x_0))^2 + \text{ Var }(\epsilon)
$$


- Due to variance and squared bias being non negative, the previous equation
implies that, to **minimize the expected test error**, we require a
statistical learnig method which achieves **low variance** and **low bias**.

- The tradeoff:
    - *Extremely low bias but high variance*: For example, draw a line which passes over every single point in the training data.
    - *Extremely low variance but high bias*: For example, fit a
    horizontal line to the data.
    - > The challenge lies in finding
      > a method for which both the variance and the squared bias are low.

- In a real-life situation, $f$ is usually unkwon, so it's not possible 
to explicitly compute the test MSE, bias or variance of a statistical method.

- The test MSE can be estimated using **cross-validation**,
but we'll discuss it later in this book.

## The Classification setting

Let's see how the concepts recently discussed change
when we the prediction is a categorical variable.

The most common approach for quantifying the accuracy
of our estimate $\hat{f}$ is the **training error rate**,
the proportion of mistakes made by applying $\hat{f}$ 
to the training observations:

$$
\dfrac{1}{n}\displaystyle{ \sum_{i=1}^{n} I(y_i \neq \hat{y}_i)}
$$

, where $I$ is $1$ when $y_i = \hat{y}_i$, and $0$ otherwise.

- The **test error rate** is defined as
$\text{ Average}(I(y_i \neq \hat{y}_i))$, where the average
is computed by comparing the predictions $\hat{y}_i$ with the
true response $y_i$. 

- A **good classifier** is one for which the test error is smallest.
